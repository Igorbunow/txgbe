# Инструкция по нагрузочному тестированию агрегированной полосы пропускания (Multi-Interface)

**Цель:** Проверить суммарную пропускную способность двух и более физических линков одновременно, чтобы выявить ограничения шины PCIe, перегрев адаптера или проблемы с распределением прерываний CPU.

## 1. Архитектура теста

Вместо объединения интерфейсов в один логический (Bonding), мы используем **разделение подсетей**. Это гарантирует, что трафик пойдет строго по назначенным физическим линиям, и позволяет загрузить все ядра CPU равномерно.

**Схема адресации (Пример):**

| Линия (Link) | Хост 1 (Sender) Interface | Хост 1 IP | <---> | Хост 2 (Receiver) Interface | Хост 2 IP | Подсеть |
| --- | --- | --- | --- | --- | --- | --- |
| **Линия A** | `enP5p1s0f0` | `10.0.1.1` | <---> | `eth2` | `10.0.1.2` | 10.0.1.0/24 |
| **Линия B** | `enP5p1s0f1` | `10.0.2.1` | <---> | `eth6` | `10.0.2.2` | 10.0.2.0/24 |

---

## 2. Подготовка системы

### 2.1. Тюнинг ядра (Sysctl)

Применяются те же настройки, что и в оригинальной инструкции. Они глобальны для всей ОС.

```bash
# Выполнить на обоих хостах
sysctl -w net.core.netdev_max_backlog=30000
sysctl -w net.core.rmem_max=134217728
sysctl -w net.core.wmem_max=134217728
sysctl -w net.core.optmem_max=20480
sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728"
sysctl -w net.ipv4.tcp_wmem="4096 65536 134217728"
sysctl -w net.ipv4.tcp_window_scaling=1

```

### 2.2. Настройка интерфейсов (с использованием `net_tune.sh`)

Используйте скрипт из Приложения А оригинальной инструкции (он уже универсален), но запустите его для каждого интерфейса с **разными** подсетями.

**На Хосте 1 (Sender):**

```bash
# Настройка первого порта (Подсеть 10.0.1.x)
./net_tune.sh enP5p1s0f0 10.0.1.1/24

# Настройка второго порта (Подсеть 10.0.2.x)
./net_tune.sh enP5p1s0f1 10.0.2.1/24

```

**На Хосте 2 (Receiver):**

```bash
# Настройка первого порта (ответная часть)
./net_tune.sh eth2 10.0.1.2/24

# Настройка второго порта (ответная часть)
./net_tune.sh eth6 10.0.2.2/24

```

---

## 3. Запуск тестирования (Parallel Streams)

Для загрузки двух каналов нам нужно запустить **два экземпляра** iperf3 одновременно. Чтобы они не конфликтовали, мы используем разные порты TCP (например, 5201 и 5202).

### 3.1. Запуск серверов (На Хосте 2 - Receiver)

Запускаем два сервера, каждый «слушает» на своем порту. Можно привязать (`-B`) их к конкретным IP для надежности.

```bash
# Сервер для Линии A (порт 5201)
iperf3 -s -B 10.0.1.2 -p 5201 &

# Сервер для Линии B (порт 5202)
iperf3 -s -B 10.0.2.2 -p 5202 &

```

### 3.2. Запуск клиентов (На Хосте 1 - Sender)

Запускаем тесты одновременно в фоновом режиме.

**Важно про CPU Affinity:**
Если вы тестируете 40G/100G, критически важно разнести процессы iperf3 по разным ядрам CPU, иначе одно ядро станет узким местом для двух мощных потоков.

```bash
# Вариант 1: Простой запуск (если скорости до 20G суммарно)
iperf3 -c 10.0.1.2 -p 5201 -t 60 -P 4 &  # Тест Линии A
iperf3 -c 10.0.2.2 -p 5202 -t 60 -P 4 &  # Тест Линии B

# Вариант 2: С привязкой к ядрам (ОБЯЗАТЕЛЬНО для 40G/100G)
# Предположим, карта на NUMA node 0. Привязываем первый тест к ядрам 0-7, второй к 8-15.
taskset -c 1-4 iperf3 -c 10.0.1.2 -p 5201 -t 60 -P 4 &
taskset -c 5-8 iperf3 -c 10.0.2.2 -p 5202 -t 60 -P 4 &

```

### 3.3. Мониторинг суммарной скорости

В отдельном терминале (на любом хосте) удобно использовать `dstat` или `sar` для просмотра суммарной загрузки сети, так как iperf3 будет выводить логи вперемешку.

```bash
# Установка dstat (если нет)
# apt install dstat / yum install dstat

# Смотрим суммарный трафик по всем интерфейсам
dstat -net

```

*Вы должны увидеть сумму скоростей (например, при двух линках 10G вы увидите "recv" или "send" около 20G).*

---

## 4. Диагностика узких мест при агрегации

При одновременной нагрузке часто возникают проблемы, которые не видны на одном порту.

### 4.1. Проверка ширины шины PCIe

Если по отдельности порты выдают 10G, а вместе только 14G (вместо 20G), проблема скорее всего в слоте PCIe.

```bash
# Проверьте статус (LnkSta) для обоих устройств или для корневого моста
lspci -vv | grep -E "LnkCap|LnkSta"

```

*Для двух портов 10G достаточно PCIe Gen2 x8 или Gen3 x4. Для двух 25G/40G уже требуется Gen3 x16 или Gen4.*

### 4.2. Пересечение прерываний (IRQ Balance)

Убедитесь, что очереди разных сетевых карт обрабатываются разными ядрами CPU.

```bash
# Просмотр прерываний для конкретного интерфейса
cat /proc/interrupts | grep enP5p1s0f0
cat /proc/interrupts | grep enP5p1s0f1

```

Если вы видите, что оба интерфейса "бомбят" прерываниями одно и то же ядро (CPU0), необходимо запустить `irqbalance` или разнести их вручную:

```bash
systemctl start irqbalance

```

---

## Приложение Б: Скрипт массового запуска (`run_dual_test.sh`)

Создайте этот скрипт на Хосте 1 (Sender) для автоматизации запуска.

```bash
#!/bin/bash

# Настройки цели
TARGET_IP_1="10.0.1.2" # IP хоста 2 на линии А
TARGET_IP_2="10.0.2.2" # IP хоста 2 на линии B
TIME=60
THREADS=4

echo "====================================================="
echo "Запуск Dual-Link теста (Total Bandwidth)"
echo "Линия A: -> $TARGET_IP_1 (Port 5201)"
echo "Линия B: -> $TARGET_IP_2 (Port 5202)"
echo "====================================================="

# Запуск первого потока в фоне (ядра 1-4)
taskset -c 1-4 iperf3 -c $TARGET_IP_1 -p 5201 -t $TIME -P $THREADS > result_link_A.txt 2>&1 &
PID1=$!

# Запуск второго потока в фоне (ядра 5-8)
taskset -c 5-8 iperf3 -c $TARGET_IP_2 -p 5202 -t $TIME -P $THREADS > result_link_B.txt 2>&1 &
PID2=$!

echo "Тесты запущены. PIDs: $PID1, $PID2"
echo "Ожидание завершения ($TIME сек)..."

wait $PID1
wait $PID2

echo "====================================================="
echo "РЕЗУЛЬТАТЫ:"
echo "--- Линия A ---"
grep "sender" result_link_A.txt | tail -n 1
echo "--- Линия B ---"
grep "sender" result_link_B.txt | tail -n 1
echo "====================================================="

```
