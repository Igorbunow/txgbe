# Инструкция по нагрузочному тестированию и диагностике SFP-интерфейсов

Данная инструкция описывает процесс подготовки операционной системы Linux и проведения тестов пропускной способности сетевых карт (10GbE и выше).

## 1. Принцип тестирования

**Цель:** Проверить физическую целостность линка, качество SFP-модулей/кабелей и максимальную пропускную способность сетевого адаптера (NIC).

**Принцип:**
Для корректного тестирования высокоскоростных интерфейсов (10G/25G/40G) необходимо исключить «узкие места» (bottlenecks), которые могут возникнуть на уровне программного обеспечения (ядра ОС и стека TCP/IP). По умолчанию настройки Linux оптимизированы для надежности и работы с множеством соединений, а не для максимальной скорости одного потока.

Мы выполняем следующие действия:

1. **Расширение буферов (Sysctl):** Увеличиваем память, выделяемую под сетевые пакеты, чтобы ядро успевало обрабатывать входящий поток без отбрасывания пакетов (drops) из-за переполнения очереди.
2. **Jumbo Frames (MTU 9000):** Увеличиваем размер полезной нагрузки в одном кадре. Это снижает количество прерываний CPU (меньше пакетов для того же объема данных) и уменьшает накладные расходы на заголовки.
3. **Offloading (Разгрузка):** Перекладываем задачи по сегментации и сборке пакетов (TSO/GSO) с центрального процессора на чип сетевой карты.
4. **Генерация трафика (Iperf3):** Запускаем синтетический тест, насыщающий канал данными.

---

## 2. Подготовка системы (Tuning)

Эти настройки необходимо применить на **обоих** хостах (Sender и Receiver).

### 2.1. Настройка ядра (Sysctl)

Команды ниже увеличивают размеры очередей и буферов TCP.

*Что делают эти параметры:*

* `net.core.netdev_max_backlog`: Очередь пакетов, которые приняты картой, но еще не обработаны ядром. Критично для 10G+, иначе пакеты будут отбрасываться при всплесках трафика.
* `rmem_max` / `wmem_max`: Максимальный размер буферов чтения/записи сокета.
* `tcp_window_scaling`: Позволяет окну TCP расти более 64Кб (необходимо для высокоскоростных сетей).

```bash
# Применение настроек на лету
sysctl -w net.core.netdev_max_backlog=30000
sysctl -w net.core.rmem_max=134217728
sysctl -w net.core.wmem_max=134217728
sysctl -w net.core.rmem_default=134217728
sysctl -w net.core.wmem_default=134217728
sysctl -w net.core.optmem_max=20480

# Настройка TCP стека (IPv4)
sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728"
sysctl -w net.ipv4.tcp_wmem="4096 65536 134217728"
sysctl -w net.ipv4.tcp_window_scaling=1
sysctl -w net.ipv4.tcp_timestamps=1
sysctl -w net.ipv4.tcp_sack=1

```

### 2.2. Настройка сетевых интерфейсов

Настройка IP-адресов, включение Jumbo Frames и аппаратной разгрузки.

*Что делают эти параметры:*

* `mtu 9000`: Включает Jumbo Frames. Важно: MTU должен быть одинаковым на обоих концах линка и на всех промежуточных коммутаторах.
* `tso on` (TCP Segmentation Offload): Карта сама режет большие данные на TCP-сегменты.
* `gso on` (Generic Segmentation Offload): Аналог TSO, но более общий.
* `sg on` (Scatter-Gather): Позволяет карте работать с фрагментированной памятью, что необходимо для TSO/GSO.

**На Хосте 1 (Пример):**

```bash
# Интерфейс 1
ip addr add 10.0.20.20/24 dev enP5p1s0f0
ip link set enP5p1s0f0 up
ip link set dev enP5p1s0f0 mtu 9000
ethtool -K enP5p1s0f0 tso on gso on sg on

# Интерфейс 2
ip addr add 10.0.21.21/24 dev enP5p1s0f1
ip link set enP5p1s0f1 up
ip link set dev enP5p1s0f1 mtu 9000
ethtool -K enP5p1s0f1 tso on gso on sg on

```

**На Хосте 2 (Пример):**

```bash
# Интерфейс 1 (ответный)
ip addr add 10.0.20.30/24 dev eth2
ip link set eth2 up
ip link set dev eth2 mtu 9000
ethtool -K eth2 tso on gso on sg on

# Интерфейс 2 (ответный)
ip addr add 10.0.21.31/24 dev eth6
ip link set eth6 up
ip link set dev eth6 mtu 9000
ethtool -K eth6 tso on gso on sg on

```

### 2.3. Настройка Firewall

Необходимо разрешить входящий трафик для теста.

```bash
# Если используется iptables
iptables -A INPUT -i eth2 -j ACCEPT
iptables -A INPUT -i eth6 -j ACCEPT
# Или полностью отключить firewall на время теста (systemctl stop firewalld / ufw disable)

```

---

## 3. Проведение тестирования

### 3.1. Проверка состояния линка

Перед запуском нагрузки убедитесь, что физика поднялась и параметры согласованы.

```bash
# Проверка скорости, дуплекса и автосогласования
ethtool enP5p1s0f0 | egrep "Speed|Duplex|Auto-neg|Link detected"

# Проверка физических параметров SFP модуля (мощность сигнала TX/RX, температура)
# ВАЖНО: Следите за RX power. Слишком низкий уровень означает плохой кабель или грязь в оптике.
ethtool -m enP5p1s0f0

```

### 3.2. Запуск мониторинга (в отдельном терминале)

Следите за ошибками в реальном времени во время теста. Рост счетчиков `drop` или `errors` говорит о проблемах.

```bash
# Очистка статистики перед тестом (некоторые драйверы не поддерживают)
ethtool -S enP5p1s0f0 > /dev/null 

# Мониторинг
watch -n 1 "ethtool -S enP5p1s0f0 | grep -E 'err|drop|loss|crc|fail'"

```

# Мониторинг сразу обоих интерфейсов

```bash
watch -n 1 "echo '--- SENDER ---'; ethtool -S enP5p1s0f0 | grep -E 'err|drop|loss|missed|crc'; echo ''; echo '--- RECEIVER ---'; ethtool -S enP5p1s0f1 | grep -E 'err|drop|loss|missed|crc'"
```

### 3.3. Запуск Iperf3 (Нагрузка)

**На сервере (принимающая сторона - Хост 1):**

```bash
iperf3 -s

```

**На клиенте (генерирующая сторона - Хост 2):**

```bash
# -c: IP сервера
# -t: время теста (30 сек)
# -P: количество параллельных потоков (критично для загрузки канала)
# -R: Reverse mode (Сервер шлет, Клиент принимает - тест в обратную сторону)
iperf3 -c 10.0.20.20 -t 30 -P 4

```

*Рекомендация:* Прогоняйте тесты в обе стороны (с ключом `-R` и без него), так как производительность TX и RX может отличаться.

---

## 4. Расширенная диагностика (Упущенные команды)

В исходных заметках отсутствуют важные команды для глубокого анализа, особенно если скорость ниже ожидаемой.

### 4.1. Привязка к NUMA-ноде (CPU Affinity)

Для скоростей 40G/100G критически важно, чтобы процесс тестирования работал на том же физическом процессоре, к которому подключена шина PCIe сетевой карты.

1. Узнать, к какой NUMA-ноде относится карта:
```bash
cat /sys/class/net/enP5p1s0f0/device/numa_node

```


*(Если вывод -1, значит система не видит NUMA или у вас один CPU).*
2. Запуск iperf с привязкой к ядрам (например, если карта на node 1):
```bash
# Привязка сервера
numactl --cpunodebind=1 iperf3 -s

# Привязка клиента
numactl --cpunodebind=1 iperf3 -c 10.0.20.20 -t 30 -P 4

```



### 4.2. Проверка Flow Control (Pause Frames)

Если карта получает "Pause Frames", это значит, что принимающая сторона или свитч просят "притормозить". Это убивает производительность.

```bash
ethtool -a enP5p1s0f0
ethtool -S enP5p1s0f0 | grep -i pause

```

### 4.3. Проверка ширины линии PCIe

Убедитесь, что карта встала в слот на нужной скорости (например, x8 Gen3 или Gen4). Если карта работает в режиме x4 или Gen1, она не выдаст полную скорость.

```bash
# LnkCap - возможности, LnkSta - текущий статус.
# Speed и Width должны совпадать или быть близкими к максимуму.
lspci -s 05:01:00.0 -vv | grep -E "LnkCap|LnkSta"

```

### 4.4. Статистика протоколов (Softnet)

Проверка, не отбрасывает ли ядро пакеты из-за перегрузки CPU (net_rx_action).

```bash
# Смотрим 2-ю и 3-ю колонки. Если значения растут во время теста - не хватает CPU.
cat /proc/net/softnet_stat

```

### 4.5. Ошибки CRC и физика (расширенный dmesg)

Ошибки AER (Advanced Error Reporting) в логах ядра указывают на проблемы с шиной PCIe или самой картой.

```bash
dmesg -T | grep -E -i "aer|pcie|warn|error|fault|eth"

```

### 4.6. Проверка нагрузки на CPU по ядрам

Во время теста важно видеть, не упирается ли одно ядро в 100% (si - softirq).

```bash
htop
# или
mpstat -P ALL 1

```

---

## 5. Чек-лист по результатам теста

1. **Скорость:** Близка ли она к теоретическому максимуму (для 10G ≈ 9.4-9.9 Gbit/s с учетом накладных расходов)?
2. **Ретрансмиты (Retr):** В выводе iperf поле `Retr` должно быть 0 или минимальным. Большое число означает потери пакетов.
3. **Drops/Errors:** Вывод `ethtool -S` не должен показывать рост счетчиков `rx_crc_errors` (битый кабель/SFP) или `rx_missed_errors` (нехватка буферов/производительности).
4. **Температура:** Температура SFP модуля (`ethtool -m`) не превышает порог (обычно 70°C).


---

Ниже представлена полная, объединенная инструкция. В неё интегрированы принципы базового тестирования и специальный раздел для проверки агрегированной пропускной способности (Dual Link), а также расширенные методы диагностики аппаратных ограничений (PCIe, температура, NUMA).

---

# Инструкция по нагрузочному тестированию SFP-интерфейсов (Single & Dual Link)

Данная инструкция описывает процесс подготовки Linux и проведения тестов пропускной способности сетевых карт (10GbE, 25GbE, 40GbE+).

## 1. Цели и принцип тестирования

Мы рассматриваем два сценария тестирования:

1. **Single Link (Одиночный тест):** Проверка качества конкретного SFP-модуля, патч-корда и отсутствия ошибок на порту.
2. **Dual Link (Агрегированный тест):** Одновременная нагрузка двух физических интерфейсов для проверки:
* Суммарной пропускной способности шины PCIe (не является ли слот «узким местом»).
* Способности CPU обрабатывать прерывания от двух портов одновременно (NUMA-балансировка).
* Температурного режима SFP-модулей при взаимном нагреве соседних портов.



**Главный принцип:** Для исключения программных ограничений мы увеличиваем сетевые буферы, включаем Jumbo Frames (MTU 9000) и аппаратные разгрузки (Offloading). Для Dual Link теста мы **не используем Bonding (LACP)**, а разносим трафик по разным подсетям для гарантированной параллельной загрузки.

---

## 2. Подготовка системы (Tuning)

Эти настройки необходимо применить на **обоих** хостах (Sender и Receiver).

### 2.1. Настройка ядра (Sysctl)

Глобальные настройки для увеличения буферов TCP и очередей ядра.

```bash
# Увеличение очередей и буферов
sysctl -w net.core.netdev_max_backlog=30000
sysctl -w net.core.rmem_max=134217728
sysctl -w net.core.wmem_max=134217728
sysctl -w net.core.optmem_max=20480

# Тюнинг TCP (IPv4)
sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728"
sysctl -w net.ipv4.tcp_wmem="4096 65536 134217728"
sysctl -w net.ipv4.tcp_window_scaling=1
sysctl -w net.ipv4.tcp_timestamps=1
sysctl -w net.ipv4.tcp_sack=1

```

### 2.2. Настройка интерфейсов (IP, MTU, Offloading)

Для настройки рекомендуется использовать скрипт `net_tune.sh` (см. **Приложение А**).
Параметры запуска зависят от выбранного сценария.

#### Схема адресации (Критически важно для Dual Link)

Чтобы Linux не маршрутизировал весь трафик через один порт, мы разносим интерфейсы по **разным подсетям**:

| Линк | Интерфейс (Пример) | IP Sender (Хост 1) | IP Receiver (Хост 2) | Подсеть |
| --- | --- | --- | --- | --- |
| **Link 1** | `enP5p1s0f0` <-> `eth2` | **10.0.20.20** | **10.0.20.30** | 10.0.20.0/24 |
| **Link 2** | `enP5p1s0f1` <-> `eth6` | **10.0.21.21** | **10.0.21.31** | 10.0.21.0/24 |

**Применение настроек (пример):**

На **Хосте 1 (Sender):**

```bash
# Настройка первого линка
./net_tune.sh enP5p1s0f0 10.0.20.20/24
# Настройка второго линка
./net_tune.sh enP5p1s0f1 10.0.21.21/24

```

На **Хосте 2 (Receiver):**

```bash
# Настройка первого линка
./net_tune.sh eth2 10.0.20.30/24
# Настройка второго линка
./net_tune.sh eth6 10.0.21.31/24

```

### 2.3. Firewall

Отключите firewall или разрешите трафик:

```bash
systemctl stop firewalld  # или ufw disable

```

---

## 3. Сценарий А: Одиночный тест (Single Link)

Используется для базовой проверки одного канала.

1. **Запуск сервера (Receiver):**
```bash
iperf3 -s

```


2. **Запуск клиента (Sender):**
```bash
# -R (Reverse) проверяет прием, без -R проверяет передачу
iperf3 -c 10.0.20.30 -t 30 -P 4

```



---

## 4. Сценарий Б: Тестирование агрегированной скорости (Dual Link)

В этом режиме мы запускаем два независимых потока `iperf3` параллельно.

### 4.1. Балансировка прерываний (NUMA и CPU Affinity)

Критически важно разнести процессы `iperf3` по разным ядрам CPU, иначе одно ядро захлебнется обработкой прерываний (softirq) от двух карт сразу.

1. **Проверка NUMA-узла:**
```bash
cat /sys/class/net/enP5p1s0f0/device/numa_node

```


*Если карты на одной NUMA-ноде (например, 0), нужно вручную делить ядра этой ноды.*
2. **Логика привязки:**
* Тест 1 -> Ядра 0-3
* Тест 2 -> Ядра 4-7



### 4.2. Запуск серверов (на Хосте 2 - Receiver)

Запускаем два экземпляра в режиме демона, каждый слушает свой IP.

```bash
# Сервер 1: Слушает IP первого линка (порт 5201)
iperf3 -s -B 10.0.20.30 -p 5201 --daemon --logfile server1.log

# Сервер 2: Слушает IP второго линка (порт 5202)
iperf3 -s -B 10.0.21.31 -p 5202 --daemon --logfile server2.log

```

### 4.3. Запуск клиентов (на Хосте 1 - Sender)

Создайте и запустите скрипт `run_dual.sh`:

```bash
#!/bin/bash
# Запуск теста Dual-Stream с привязкой к ядрам

echo "Запуск тестов..."

# Тест 1: Идет на 10.0.20.30 (Link 1). Ядра 0-3.
taskset -c 0-3 iperf3 -c 10.0.20.30 -p 5201 -B 10.0.20.20 -t 60 -P 4 --logfile client1.log &
PID1=$!

# Тест 2: Идет на 10.0.21.31 (Link 2). Ядра 4-7.
taskset -c 4-7 iperf3 -c 10.0.21.31 -p 5202 -B 10.0.21.21 -t 60 -P 4 --logfile client2.log &
PID2=$!

echo "Процессы запущены (PIDs: $PID1, $PID2). Ожидание 60 секунд..."
wait

echo "--- РЕЗУЛЬТАТЫ ---"
# Вывод итогов (парсим логи)
grep "receiver" client1.log | tail -n 1 | awk '{print "Link 1 (Receiver): " $7 " " $8}'
grep "sender"   client1.log | tail -n 1 | awk '{print "Link 1 (Sender):   " $7 " " $8}'
echo "------------------"
grep "receiver" client2.log | tail -n 1 | awk '{print "Link 2 (Receiver): " $7 " " $8}'
grep "sender"   client2.log | tail -n 1 | awk '{print "Link 2 (Sender):   " $7 " " $8}'

```

---

## 5. Мониторинг и Диагностика

Во время теста (особенно Dual Link) необходимо следить за состоянием "железа".

### 5.1. Мониторинг ошибок и потерь (Real-time)

Запустите в отдельном окне. Команда выводит статистику сразу по двум интерфейсам:

```bash
watch -n 1 "echo '=== LINK 1 ==='; ethtool -S enP5p1s0f0 | grep -E 'err|drop|loss|crc|missed'; echo ''; echo '=== LINK 2 ==='; ethtool -S enP5p1s0f1 | grep -E 'err|drop|loss|crc|missed'"

```

* `crc_errors`: Проблема физики (кабель, модуль, грязь).
* `missed_errors` / `dropped`: Нехватка производительности CPU или переполнение буферов.

### 5.2. Температурный контроль (Взаимный нагрев)

При полной загрузке двух портов модули могут перегреваться.
Сравнение температур двух модулей одной командой:

```bash
paste <(ethtool -m enP5p1s0f0 | grep Temper) <(ethtool -m enP5p1s0f1 | grep Temper)

```

*Температура выше 70°C считается критической для большинства коммерческих SFP.*

### 5.3. Проверка просадки скорости (PCIe Bottleneck)

Если сумма скоростей в Dual тесте (например, 14 Gbit/s) значительно ниже суммы двух одиночных тестов (10 + 10 = 20 Gbit/s), проверьте ширину шины PCIe.

```bash
# LnkSta: Speed и Width (должно быть x8 или x16, Gen3 или Gen4)
lspci -s <BUS_ID> -vv | grep -E "LnkCap|LnkSta"

```

### 5.4. Загрузка CPU (SoftIRQ)

Запустите `htop` во время теста.

* Если одно ядро загружено на 100% красным цветом (`si` - software interrupts), значит, прерывания от двух карт падают на одно ядро. Требуется ручная балансировка (`set_irq_affinity`).

---

## Приложение А: Скрипт `net_tune.sh`

Сохраните как `net_tune.sh` и сделайте исполняемым (`chmod +x net_tune.sh`).

```bash
#!/bin/bash
# Использование: ./net_tune.sh <INTERFACE> <IP/CIDR>
# Пример: ./net_tune.sh enP5p1s0f0 10.0.20.20/24

if [[ $EUID -ne 0 ]]; then echo "Run as root"; exit 1; fi
if [ -z "$1" ] || [ -z "$2" ]; then echo "Usage: $0 <iface> <ip/cidr>"; exit 1; fi

IFACE=$1
IP_CIDR=$2

echo ">>> Настройка $IFACE [$IP_CIDR]..."

# 1. Сброс и установка IP
ip addr flush dev $IFACE
ip addr add $IP_CIDR dev $IFACE
ip link set $IFACE up

# 2. MTU 9000 (Jumbo Frames)
ip link set dev $IFACE mtu 9000
if [ $? -eq 0 ]; then echo "MTU 9000: OK"; else echo "MTU 9000: FAIL"; fi

# 3. Offloading
ethtool -K $IFACE tso on gso on sg on gro on > /dev/null 2>&1
echo "Offloading (TSO/GSO/GRO): Enabled"

# 4. Проверка
SPEED=$(ethtool $IFACE | grep "Speed" | awk '{print $2}')
LINK=$(ethtool $IFACE | grep "Link detected" | awk '{print $3}')

echo "Статус: Link=$LINK, Speed=$SPEED"
echo "----------------------------------------"

```

### Приложение А: Скрипт инициализации сетевого интерфейса (`net_tune.sh`)

Ниже представлен универсальный Bash-скрипт, который автоматизирует процесс настройки. Он объединяет тюнинг ядра, настройку IP-адресации, MTU и включение аппаратных разгрузок (offloading).


Скрипт написан так, чтобы его можно было запускать на любом хосте (и на Sender, и на Receiver), просто меняя аргументы.

Сохраните этот код в файл, например `net_tune.sh`, и сделайте его исполняемым: `chmod +x net_tune.sh`.

```bash
#!/bin/bash

# ==============================================================================
# SFP/10G+ Network Interface Tuning Script
# Описание: Скрипт для подготовки интерфейса к нагрузочному тестированию.
#           Применяет настройки sysctl, MTU, IP и Offloading.
#
# Использование: ./net_tune.sh <INTERFACE_NAME> <IP_ADDRESS_CIDR>
# Пример:        ./net_tune.sh enP5p1s0f0 10.0.20.20/24
# ==============================================================================

# Проверка прав суперпользователя
if [[ $EUID -ne 0 ]]; then
   echo "Ошибка: Этот скрипт должен быть запущен от имени root."
   exit 1
fi

# Проверка аргументов
if [ -z "$1" ] || [ -z "$2" ]; then
    echo "Использование: $0 <интерфейс> <IP-адрес/маска>"
    echo "Пример: $0 eth2 192.168.100.2/24"
    exit 1
fi

IFACE=$1
IP_CIDR=$2

# Проверка существования интерфейса
if [ ! -d "/sys/class/net/$IFACE" ]; then
    echo "Ошибка: Интерфейс $IFACE не найден в системе."
    exit 1
fi

echo "============================================================"
echo " ЗАПУСК НАСТРОЙКИ ДЛЯ ИНТЕРФЕЙСА: $IFACE"
echo "============================================================"

# ------------------------------------------------------------------------------
# 1. Тюнинг ядра (Sysctl)
# ------------------------------------------------------------------------------
echo "[1/4] Применение параметров Sysctl (Tuning Kernel)..."

# Увеличиваем очередь входящих пакетов на уровне драйвера
sysctl -w net.core.netdev_max_backlog=30000 > /dev/null

# Увеличиваем максимальные размеры буферов сокетов (ОС)
sysctl -w net.core.rmem_max=134217728 > /dev/null
sysctl -w net.core.wmem_max=134217728 > /dev/null
sysctl -w net.core.rmem_default=134217728 > /dev/null
sysctl -w net.core.wmem_default=134217728 > /dev/null
sysctl -w net.core.optmem_max=20480 > /dev/null

# Тюнинг TCP стека (IPv4)
# Low / Pressure / High thresholds
sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728" > /dev/null
sysctl -w net.ipv4.tcp_wmem="4096 65536 134217728" > /dev/null

# Включение масштабирования окна и SACK
sysctl -w net.ipv4.tcp_window_scaling=1 > /dev/null
sysctl -w net.ipv4.tcp_timestamps=1 > /dev/null
sysctl -w net.ipv4.tcp_sack=1 > /dev/null

# Отключаем фильтрацию обратного пути (для тестов полезно, чтобы избежать проблем с роутингом)
sysctl -w net.ipv4.conf.all.rp_filter=0 > /dev/null
sysctl -w net.ipv4.conf.$IFACE.rp_filter=0 > /dev/null

echo "      -> Параметры ядра применены."

# ------------------------------------------------------------------------------
# 2. Настройка IP и MTU
# ------------------------------------------------------------------------------
echo "[2/4] Настройка IP адреса и MTU..."

# Сброс старых IP адресов на интерфейсе (чтобы не было конфликтов)
ip addr flush dev $IFACE

# Установка нового IP
ip addr add $IP_CIDR dev $IFACE
if [ $? -eq 0 ]; then
    echo "      -> IP адрес $IP_CIDR установлен."
else
    echo "      -> ОШИБКА установки IP адреса."
    exit 1
fi

# Поднятие интерфейса
ip link set $IFACE up

# Установка Jumbo Frames (MTU 9000)
# ВАЖНО: Убедитесь, что свитч между хостами тоже поддерживает MTU 9000
ip link set dev $IFACE mtu 9000
if [ $? -eq 0 ]; then
    echo "      -> MTU установлен в 9000 (Jumbo Frames)."
else
    echo "      -> ОШИБКА: Не удалось установить MTU 9000. Проверьте поддержку драйвером."
fi

# ------------------------------------------------------------------------------
# 3. Настройка Offloading (Разгрузка CPU)
# ------------------------------------------------------------------------------
echo "[3/4] Включение аппаратной разгрузки (Offloading)..."

# TSO: TCP Segmentation Offload
# GSO: Generic Segmentation Offload
# SG:  Scatter-Gather
# GRO: Generic Receive Offload
ethtool -K $IFACE tso on gso on sg on gro on > /dev/null 2>&1

echo "      -> TSO, GSO, SG, GRO активированы."

# ------------------------------------------------------------------------------
# 4. Диагностика и проверка
# ------------------------------------------------------------------------------
echo "[4/4] Проверка статуса..."
echo "------------------------------------------------------------"

# Вывод информации о NUMA-ноде (важно для привязки процессов)
if [ -f "/sys/class/net/$IFACE/device/numa_node" ]; then
    NUMA_NODE=$(cat /sys/class/net/$IFACE/device/numa_node)
    echo "INFO: Интерфейс привязан к NUMA Node: $NUMA_NODE"
    echo "      (Используйте 'numactl --cpunodebind=$NUMA_NODE' для запуска iperf3)"
else
    echo "INFO: NUMA Node не определен."
fi

# Проверка линка
LINK_STATUS=$(ethtool $IFACE | grep "Link detected" | awk '{print $3}')
SPEED=$(ethtool $IFACE | grep "Speed" | awk '{print $2}')
DUPLEX=$(ethtool $IFACE | grep "Duplex" | awk '{print $2}')

echo "INFO: Статус линка: $LINK_STATUS"
echo "INFO: Скорость:     $SPEED"
echo "INFO: Дуплекс:      $DUPLEX"
echo "INFO: Текущий MTU:  $(cat /sys/class/net/$IFACE/mtu)"

echo "------------------------------------------------------------"
echo "Готово. Можно запускать тесты."
echo ""
echo "Команда для мониторинга ошибок:"
echo "watch -n 1 \"ethtool -S $IFACE | grep -E 'err|drop|loss'\""
echo "============================================================"

```

---

### Как пользоваться этим скриптом

1. **На Хосте 1 (например, Сервер):**
```bash
# Настраиваем интерфейс enP5p1s0f0 с адресом 10.0.20.20
./net_tune.sh enP5p1s0f0 10.0.20.20/24

# После успешного выполнения запускаем iperf сервер
# (допустим, скрипт сказал, что NUMA Node: 0)
numactl --cpunodebind=0 iperf3 -s

```


2. **На Хосте 2 (например, Клиент):**
```bash
# Настраиваем интерфейс eth2 с адресом 10.0.20.30
./net_tune.sh eth2 10.0.20.30/24

# Запускаем тест
# (допустим, скрипт сказал, что NUMA Node: 1)
numactl --cpunodebind=1 iperf3 -c 10.0.20.20 -t 30 -P 4

```

