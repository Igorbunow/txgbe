# Инструкция по нагрузочному тестированию и диагностике SFP-интерфейсов

Данная инструкция описывает процесс подготовки операционной системы Linux и проведения тестов пропускной способности сетевых карт (10GbE и выше).

## 1. Принцип тестирования

**Цель:** Проверить физическую целостность линка, качество SFP-модулей/кабелей и максимальную пропускную способность сетевого адаптера (NIC).

**Принцип:**
Для корректного тестирования высокоскоростных интерфейсов (10G/25G/40G) необходимо исключить «узкие места» (bottlenecks), которые могут возникнуть на уровне программного обеспечения (ядра ОС и стека TCP/IP). По умолчанию настройки Linux оптимизированы для надежности и работы с множеством соединений, а не для максимальной скорости одного потока.

Мы выполняем следующие действия:

1. **Расширение буферов (Sysctl):** Увеличиваем память, выделяемую под сетевые пакеты, чтобы ядро успевало обрабатывать входящий поток без отбрасывания пакетов (drops) из-за переполнения очереди.
2. **Jumbo Frames (MTU 9000):** Увеличиваем размер полезной нагрузки в одном кадре. Это снижает количество прерываний CPU (меньше пакетов для того же объема данных) и уменьшает накладные расходы на заголовки.
3. **Offloading (Разгрузка):** Перекладываем задачи по сегментации и сборке пакетов (TSO/GSO) с центрального процессора на чип сетевой карты.
4. **Генерация трафика (Iperf3):** Запускаем синтетический тест, насыщающий канал данными.

---

## 2. Подготовка системы (Tuning)

Эти настройки необходимо применить на **обоих** хостах (Sender и Receiver).

### 2.1. Настройка ядра (Sysctl)

Команды ниже увеличивают размеры очередей и буферов TCP.

*Что делают эти параметры:*

* `net.core.netdev_max_backlog`: Очередь пакетов, которые приняты картой, но еще не обработаны ядром. Критично для 10G+, иначе пакеты будут отбрасываться при всплесках трафика.
* `rmem_max` / `wmem_max`: Максимальный размер буферов чтения/записи сокета.
* `tcp_window_scaling`: Позволяет окну TCP расти более 64Кб (необходимо для высокоскоростных сетей).

```bash
# Применение настроек на лету
sysctl -w net.core.netdev_max_backlog=30000
sysctl -w net.core.rmem_max=134217728
sysctl -w net.core.wmem_max=134217728
sysctl -w net.core.rmem_default=134217728
sysctl -w net.core.wmem_default=134217728
sysctl -w net.core.optmem_max=20480

# Настройка TCP стека (IPv4)
sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728"
sysctl -w net.ipv4.tcp_wmem="4096 65536 134217728"
sysctl -w net.ipv4.tcp_window_scaling=1
sysctl -w net.ipv4.tcp_timestamps=1
sysctl -w net.ipv4.tcp_sack=1

```

### 2.2. Настройка сетевых интерфейсов

Настройка IP-адресов, включение Jumbo Frames и аппаратной разгрузки.

*Что делают эти параметры:*

* `mtu 9000`: Включает Jumbo Frames. Важно: MTU должен быть одинаковым на обоих концах линка и на всех промежуточных коммутаторах.
* `tso on` (TCP Segmentation Offload): Карта сама режет большие данные на TCP-сегменты.
* `gso on` (Generic Segmentation Offload): Аналог TSO, но более общий.
* `sg on` (Scatter-Gather): Позволяет карте работать с фрагментированной памятью, что необходимо для TSO/GSO.

**На Хосте 1 (Пример):**

```bash
# Интерфейс 1
ip addr add 10.0.20.20/24 dev enP5p1s0f0
ip link set enP5p1s0f0 up
ip link set dev enP5p1s0f0 mtu 9000
ethtool -K enP5p1s0f0 tso on gso on sg on

# Интерфейс 2
ip addr add 10.0.21.21/24 dev enP5p1s0f1
ip link set enP5p1s0f1 up
ip link set dev enP5p1s0f1 mtu 9000
ethtool -K enP5p1s0f1 tso on gso on sg on

```

**На Хосте 2 (Пример):**

```bash
# Интерфейс 1 (ответный)
ip addr add 10.0.20.30/24 dev eth2
ip link set eth2 up
ip link set dev eth2 mtu 9000
ethtool -K eth2 tso on gso on sg on

# Интерфейс 2 (ответный)
ip addr add 10.0.21.31/24 dev eth6
ip link set eth6 up
ip link set dev eth6 mtu 9000
ethtool -K eth6 tso on gso on sg on

```

### 2.3. Настройка Firewall

Необходимо разрешить входящий трафик для теста.

```bash
# Если используется iptables
iptables -A INPUT -i eth2 -j ACCEPT
iptables -A INPUT -i eth6 -j ACCEPT
# Или полностью отключить firewall на время теста (systemctl stop firewalld / ufw disable)

```

---

## 3. Проведение тестирования

### 3.1. Проверка состояния линка

Перед запуском нагрузки убедитесь, что физика поднялась и параметры согласованы.

```bash
# Проверка скорости, дуплекса и автосогласования
ethtool enP5p1s0f0 | egrep "Speed|Duplex|Auto-neg|Link detected"

# Проверка физических параметров SFP модуля (мощность сигнала TX/RX, температура)
# ВАЖНО: Следите за RX power. Слишком низкий уровень означает плохой кабель или грязь в оптике.
ethtool -m enP5p1s0f0

```

### 3.2. Запуск мониторинга (в отдельном терминале)

Следите за ошибками в реальном времени во время теста. Рост счетчиков `drop` или `errors` говорит о проблемах.

```bash
# Очистка статистики перед тестом (некоторые драйверы не поддерживают)
ethtool -S enP5p1s0f0 > /dev/null 

# Мониторинг
watch -n 1 "ethtool -S enP5p1s0f0 | grep -E 'err|drop|loss|crc|fail'"

```

# Мониторинг сразу обоих интерфейсов

```bash
watch -n 1 "echo '--- SENDER ---'; ethtool -S enP5p1s0f0 | grep -E 'err|drop|loss|missed|crc'; echo ''; echo '--- RECEIVER ---'; ethtool -S enP5p1s0f1 | grep -E 'err|drop|loss|missed|crc'"
```

### 3.3. Запуск Iperf3 (Нагрузка)

**На сервере (принимающая сторона - Хост 1):**

```bash
iperf3 -s

```

**На клиенте (генерирующая сторона - Хост 2):**

```bash
# -c: IP сервера
# -t: время теста (30 сек)
# -P: количество параллельных потоков (критично для загрузки канала)
# -R: Reverse mode (Сервер шлет, Клиент принимает - тест в обратную сторону)
iperf3 -c 10.0.20.20 -t 30 -P 4

```

*Рекомендация:* Прогоняйте тесты в обе стороны (с ключом `-R` и без него), так как производительность TX и RX может отличаться.

---

## 4. Расширенная диагностика (Упущенные команды)

В исходных заметках отсутствуют важные команды для глубокого анализа, особенно если скорость ниже ожидаемой.

### 4.1. Привязка к NUMA-ноде (CPU Affinity)

Для скоростей 40G/100G критически важно, чтобы процесс тестирования работал на том же физическом процессоре, к которому подключена шина PCIe сетевой карты.

1. Узнать, к какой NUMA-ноде относится карта:
```bash
cat /sys/class/net/enP5p1s0f0/device/numa_node

```


*(Если вывод -1, значит система не видит NUMA или у вас один CPU).*
2. Запуск iperf с привязкой к ядрам (например, если карта на node 1):
```bash
# Привязка сервера
numactl --cpunodebind=1 iperf3 -s

# Привязка клиента
numactl --cpunodebind=1 iperf3 -c 10.0.20.20 -t 30 -P 4

```



### 4.2. Проверка Flow Control (Pause Frames)

Если карта получает "Pause Frames", это значит, что принимающая сторона или свитч просят "притормозить". Это убивает производительность.

```bash
ethtool -a enP5p1s0f0
ethtool -S enP5p1s0f0 | grep -i pause

```

### 4.3. Проверка ширины линии PCIe

Убедитесь, что карта встала в слот на нужной скорости (например, x8 Gen3 или Gen4). Если карта работает в режиме x4 или Gen1, она не выдаст полную скорость.

```bash
# LnkCap - возможности, LnkSta - текущий статус.
# Speed и Width должны совпадать или быть близкими к максимуму.
lspci -s 05:01:00.0 -vv | grep -E "LnkCap|LnkSta"

```

### 4.4. Статистика протоколов (Softnet)

Проверка, не отбрасывает ли ядро пакеты из-за перегрузки CPU (net_rx_action).

```bash
# Смотрим 2-ю и 3-ю колонки. Если значения растут во время теста - не хватает CPU.
cat /proc/net/softnet_stat

```

### 4.5. Ошибки CRC и физика (расширенный dmesg)

Ошибки AER (Advanced Error Reporting) в логах ядра указывают на проблемы с шиной PCIe или самой картой.

```bash
dmesg -T | grep -E -i "aer|pcie|warn|error|fault|eth"

```

### 4.6. Проверка нагрузки на CPU по ядрам

Во время теста важно видеть, не упирается ли одно ядро в 100% (si - softirq).

```bash
htop
# или
mpstat -P ALL 1

```

---

## 5. Чек-лист по результатам теста

1. **Скорость:** Близка ли она к теоретическому максимуму (для 10G ≈ 9.4-9.9 Gbit/s с учетом накладных расходов)?
2. **Ретрансмиты (Retr):** В выводе iperf поле `Retr` должно быть 0 или минимальным. Большое число означает потери пакетов.
3. **Drops/Errors:** Вывод `ethtool -S` не должен показывать рост счетчиков `rx_crc_errors` (битый кабель/SFP) или `rx_missed_errors` (нехватка буферов/производительности).
4. **Температура:** Температура SFP модуля (`ethtool -m`) не превышает порог (обычно 70°C).


---

# Дополнение: Тестирование агрегированной пропускной способности (Dual Link)

**Цель:** Прогнать трафик одновременно через два физических интерфейса (например, `enP5p1s0f0` + `enP5p1s0f1`), чтобы проверить:

1. Суммарную пропускную способность шины PCIe (не упираемся ли мы в слот).
2. Способность CPU обрабатывать прерывания от двух портов (NUMA-балансировка).
3. Температурный режим SFP-модулей при полной загрузке соседних портов (взаимный нагрев).

## 1. Схема адресации (Критически важно)

Чтобы Linux не пытался отправить весь трафик через один порт (маршрутизация по умолчанию), необходимо разнести интерфейсы по **разным подсетям**.

**Схема:**

* **Link 1 (Подсеть 10.0.20.x):**
* Sender (enP5p1s0f0): `10.0.20.20`
* Receiver (eth2): `10.0.20.30`


* **Link 2 (Подсеть 10.0.21.x):**
* Sender (enP5p1s0f1): `10.0.21.21`
* Receiver (eth6): `10.0.21.31`



## 2. Подготовка (Tuning)

Используйте приложенный скрипт `net_tune.sh` для каждого интерфейса по очереди.

**На Хосте 1 (Sender):**

```bash
./net_tune.sh enP5p1s0f0 10.0.20.20/24
./net_tune.sh enP5p1s0f1 10.0.21.21/24

```

**На Хосте 2 (Receiver):**

```bash
./net_tune.sh eth2 10.0.20.30/24
./net_tune.sh eth6 10.0.21.31/24

```

## 3. Балансировка прерываний (NUMA)

При запуске двух потоков критически важно разнести процессы `iperf3` по разным ядрам CPU, иначе одно ядро захлебнется обработкой softirq от двух сетевых карт.

1. Проверьте NUMA-узлы для карт:
```bash
cat /sys/class/net/enP5p1s0f0/device/numa_node
cat /sys/class/net/enP5p1s0f1/device/numa_node

```


2. Если они на одной NUMA-ноде (например, 0), привязывайте процессы к **разным физическим ядрам** внутри этой ноды (например, ядра 1-4 для первого теста, 5-8 для второго).

## 4. Запуск тестирования

Нам нужно запустить два сервера и два клиента одновременно.

### Шаг 4.1. Запуск серверов (Receiver - Хост 1)

Запускаем два экземпляра `iperf3`, привязанных к конкретным IP-адресам (ключ `-B`), чтобы они слушали разные интерфейсы. Используем разные порты для порядка (хотя при разных IP можно оставить 5201, но лучше разнести).

```bash
# Сервер 1: Слушает на IP первого линка, порт 5201
iperf3 -s -B 10.0.20.20 -p 5201 --daemon --logfile server1.log

# Сервер 2: Слушает на IP второго линка, порт 5202
iperf3 -s -B 10.0.21.21 -p 5202 --daemon --logfile server2.log

```

*Флаг `--daemon` запустит процесс в фоне. Логи будут писаться в файлы.*

### Шаг 4.2. Запуск клиентов (Sender - Хост 2)

Запускаем тесты параллельно в фоновом режиме `&` и ждем их завершения.

```bash
#!/bin/bash
# Запуск теста Dual-Stream

# Тест 1: Идет на 10.0.20.20 (через eth2 -> enP5p1s0f0)
# Привязываем к ядрам 0-3 (пример)
taskset -c 0-3 iperf3 -c 10.0.20.20 -p 5201 -B 10.0.20.30 -t 60 -P 4 --logfile client1.log &

# Тест 2: Идет на 10.0.21.21 (через eth6 -> enP5p1s0f1)
# Привязываем к ядрам 4-7 (пример), чтобы не мешать первому тесту
taskset -c 4-7 iperf3 -c 10.0.21.21 -p 5202 -B 10.0.21.31 -t 60 -P 4 --logfile client2.log &

echo "Тесты запущены. Ожидание 60 секунд..."
wait
echo "Тесты завершены."

# Вывод итогов (суммируем Receiver bitrates)
echo "--- РЕЗУЛЬТАТЫ ---"
grep "receiver" client1.log | tail -n 1 | awk '{print "Link 1: " $7 " " $8}'
grep "receiver" client2.log | tail -n 1 | awk '{print "Link 2: " $7 " " $8}'


```

### Шаг 4.3. Мониторинг во время теста

Чтобы видеть суммарную загрузку, используйте скрипт мониторинга для двух интерфейсов сразу.

```bash
watch -n 1 "echo '=== LINK 1 (enP5p1s0f0) ==='; ethtool -S enP5p1s0f0 | grep -E 'err|drop|loss|crc'; echo ''; echo '=== LINK 2 (enP5p1s0f1) ==='; ethtool -S enP5p1s0f1 | grep -E 'err|drop|loss|crc'"

```

---

## 5. Возможные проблемы при агрегации

1. **Просадка суммарной скорости:**
* Если по отдельности каждый линк дает 10G, а вместе — 14G (вместо 20G), проверьте **PCIe**. Возможно, слот работает в режиме x4 вместо x8, или обе карты делят узкую шину DMI (если это десктопная плата).
* Используйте `lspci -tv`, чтобы посмотреть дерево устройств.


2. **Перегрев:**
* При одновременной нагрузке двух портов SFP модули греют друг друга. Обязательно проверяйте температуру:


```bash
paste <(ethtool -m enP5p1s0f0 | grep Temper) <(ethtool -m enP5p1s0f1 | grep Temper)

```


3. **CPU Bottleneck:**
* Если `htop` показывает, что одно ядро (Core) загружено на 100% в режиме `si` (softirq), значит, оба сетевых адаптера "сели" на одно ядро. Используйте `set_irq_affinity` (часто идет с драйверами Intel/Mellanox) или вручную раскидайте прерывания через `/proc/irq/`.


### Приложение А: Скрипт инициализации сетевого интерфейса (`net_tune.sh`)

Ниже представлен универсальный Bash-скрипт, который автоматизирует процесс настройки. Он объединяет тюнинг ядра, настройку IP-адресации, MTU и включение аппаратных разгрузок (offloading).


Скрипт написан так, чтобы его можно было запускать на любом хосте (и на Sender, и на Receiver), просто меняя аргументы.

Сохраните этот код в файл, например `net_tune.sh`, и сделайте его исполняемым: `chmod +x net_tune.sh`.

```bash
#!/bin/bash

# ==============================================================================
# SFP/10G+ Network Interface Tuning Script
# Описание: Скрипт для подготовки интерфейса к нагрузочному тестированию.
#           Применяет настройки sysctl, MTU, IP и Offloading.
#
# Использование: ./net_tune.sh <INTERFACE_NAME> <IP_ADDRESS_CIDR>
# Пример:        ./net_tune.sh enP5p1s0f0 10.0.20.20/24
# ==============================================================================

# Проверка прав суперпользователя
if [[ $EUID -ne 0 ]]; then
   echo "Ошибка: Этот скрипт должен быть запущен от имени root."
   exit 1
fi

# Проверка аргументов
if [ -z "$1" ] || [ -z "$2" ]; then
    echo "Использование: $0 <интерфейс> <IP-адрес/маска>"
    echo "Пример: $0 eth2 192.168.100.2/24"
    exit 1
fi

IFACE=$1
IP_CIDR=$2

# Проверка существования интерфейса
if [ ! -d "/sys/class/net/$IFACE" ]; then
    echo "Ошибка: Интерфейс $IFACE не найден в системе."
    exit 1
fi

echo "============================================================"
echo " ЗАПУСК НАСТРОЙКИ ДЛЯ ИНТЕРФЕЙСА: $IFACE"
echo "============================================================"

# ------------------------------------------------------------------------------
# 1. Тюнинг ядра (Sysctl)
# ------------------------------------------------------------------------------
echo "[1/4] Применение параметров Sysctl (Tuning Kernel)..."

# Увеличиваем очередь входящих пакетов на уровне драйвера
sysctl -w net.core.netdev_max_backlog=30000 > /dev/null

# Увеличиваем максимальные размеры буферов сокетов (ОС)
sysctl -w net.core.rmem_max=134217728 > /dev/null
sysctl -w net.core.wmem_max=134217728 > /dev/null
sysctl -w net.core.rmem_default=134217728 > /dev/null
sysctl -w net.core.wmem_default=134217728 > /dev/null
sysctl -w net.core.optmem_max=20480 > /dev/null

# Тюнинг TCP стека (IPv4)
# Low / Pressure / High thresholds
sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728" > /dev/null
sysctl -w net.ipv4.tcp_wmem="4096 65536 134217728" > /dev/null

# Включение масштабирования окна и SACK
sysctl -w net.ipv4.tcp_window_scaling=1 > /dev/null
sysctl -w net.ipv4.tcp_timestamps=1 > /dev/null
sysctl -w net.ipv4.tcp_sack=1 > /dev/null

# Отключаем фильтрацию обратного пути (для тестов полезно, чтобы избежать проблем с роутингом)
sysctl -w net.ipv4.conf.all.rp_filter=0 > /dev/null
sysctl -w net.ipv4.conf.$IFACE.rp_filter=0 > /dev/null

echo "      -> Параметры ядра применены."

# ------------------------------------------------------------------------------
# 2. Настройка IP и MTU
# ------------------------------------------------------------------------------
echo "[2/4] Настройка IP адреса и MTU..."

# Сброс старых IP адресов на интерфейсе (чтобы не было конфликтов)
ip addr flush dev $IFACE

# Установка нового IP
ip addr add $IP_CIDR dev $IFACE
if [ $? -eq 0 ]; then
    echo "      -> IP адрес $IP_CIDR установлен."
else
    echo "      -> ОШИБКА установки IP адреса."
    exit 1
fi

# Поднятие интерфейса
ip link set $IFACE up

# Установка Jumbo Frames (MTU 9000)
# ВАЖНО: Убедитесь, что свитч между хостами тоже поддерживает MTU 9000
ip link set dev $IFACE mtu 9000
if [ $? -eq 0 ]; then
    echo "      -> MTU установлен в 9000 (Jumbo Frames)."
else
    echo "      -> ОШИБКА: Не удалось установить MTU 9000. Проверьте поддержку драйвером."
fi

# ------------------------------------------------------------------------------
# 3. Настройка Offloading (Разгрузка CPU)
# ------------------------------------------------------------------------------
echo "[3/4] Включение аппаратной разгрузки (Offloading)..."

# TSO: TCP Segmentation Offload
# GSO: Generic Segmentation Offload
# SG:  Scatter-Gather
# GRO: Generic Receive Offload
ethtool -K $IFACE tso on gso on sg on gro on > /dev/null 2>&1

echo "      -> TSO, GSO, SG, GRO активированы."

# ------------------------------------------------------------------------------
# 4. Диагностика и проверка
# ------------------------------------------------------------------------------
echo "[4/4] Проверка статуса..."
echo "------------------------------------------------------------"

# Вывод информации о NUMA-ноде (важно для привязки процессов)
if [ -f "/sys/class/net/$IFACE/device/numa_node" ]; then
    NUMA_NODE=$(cat /sys/class/net/$IFACE/device/numa_node)
    echo "INFO: Интерфейс привязан к NUMA Node: $NUMA_NODE"
    echo "      (Используйте 'numactl --cpunodebind=$NUMA_NODE' для запуска iperf3)"
else
    echo "INFO: NUMA Node не определен."
fi

# Проверка линка
LINK_STATUS=$(ethtool $IFACE | grep "Link detected" | awk '{print $3}')
SPEED=$(ethtool $IFACE | grep "Speed" | awk '{print $2}')
DUPLEX=$(ethtool $IFACE | grep "Duplex" | awk '{print $2}')

echo "INFO: Статус линка: $LINK_STATUS"
echo "INFO: Скорость:     $SPEED"
echo "INFO: Дуплекс:      $DUPLEX"
echo "INFO: Текущий MTU:  $(cat /sys/class/net/$IFACE/mtu)"

echo "------------------------------------------------------------"
echo "Готово. Можно запускать тесты."
echo ""
echo "Команда для мониторинга ошибок:"
echo "watch -n 1 \"ethtool -S $IFACE | grep -E 'err|drop|loss'\""
echo "============================================================"

```

---

### Как пользоваться этим скриптом

1. **На Хосте 1 (например, Сервер):**
```bash
# Настраиваем интерфейс enP5p1s0f0 с адресом 10.0.20.20
./net_tune.sh enP5p1s0f0 10.0.20.20/24

# После успешного выполнения запускаем iperf сервер
# (допустим, скрипт сказал, что NUMA Node: 0)
numactl --cpunodebind=0 iperf3 -s

```


2. **На Хосте 2 (например, Клиент):**
```bash
# Настраиваем интерфейс eth2 с адресом 10.0.20.30
./net_tune.sh eth2 10.0.20.30/24

# Запускаем тест
# (допустим, скрипт сказал, что NUMA Node: 1)
numactl --cpunodebind=1 iperf3 -c 10.0.20.20 -t 30 -P 4

```

